@ARTICLE{8085174, 
author={S. Zhang and S. Zhang and T. Huang and W. Gao}, 
journal={IEEE Transactions on Multimedia}, 
title={Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching}, 
year={2017}, 
volume={PP}, 
number={99},
pages={1-1}, 
keywords={Acoustics;Convolution;Emotion recognition;Feature extraction;Neural networks;Speech;Speech recognition;Deep Convolutional Neural Network;Discriminant Temporal Pyramid Matching;Feature Learning;Lp-norm Pooling;Speech Emotion Recognition}, 
doi={10.1109/TMM.2017.2766843}, 
ISSN={1520-9210}, 
month={},}

@ARTICLE{7956190, 
author={S. Zhang and S. Zhang and T. Huang and W. Gao and Q. Tian}, 
journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
title={Learning Affective Features with a Hybrid Deep Model for Audio-Visual Emotion Recognition}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
keywords={Convolution;Databases;Emotion recognition;Feature extraction;Image segmentation;Machine learning;Visualization;Emotion recognition;convolutional neural networks;deep belief networks;deep learning;multimodality fusion}, 
doi={10.1109/TCSVT.2017.2719043}, 
ISSN={1051-8215}, 
month={},}

@ARTICLE{7654321, 
author={Mohamed, Abdel-rahman},
journal = {University of Toronto},
title={Deep Neural Network Acoustic Models for ASR}, 
year={2014},
keywords={Deep Neural Networks; Acoustic models; Automatic speech recognition; Machine learning}, 
url = {http://hdl.handle.net/1807/44123}
}

@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@INPROCEEDINGS{5995370,
author={J. Feng and B. Ni and Q. Tian and S. Yan},
booktitle={CVPR 2011},
title={Geometric Lp-norm feature pooling for image classification},
year={2011},
volume={},
number={},
pages={2609-2704},
keywords={image classification;statistical analysis;average poolings;class-specific feature spatial distribution;feature spatial correlation;geometric â„“p-norm feature pooling;image classification;local features;max poolings;spatial pooling operation;statistical summarization;visual classification models;weighted spatial pooling function;Correlation;Dictionaries;Encoding;Feature extraction;Indexes;Vectors;Visualization},
doi={10.1109/CVPR.2011.5995370},
ISSN={1063-6919},
month={June},}

@ARTICLE{4016549, 
author={S. Yan and D. Xu and B. Zhang and H. j. Zhang and Q. Yang and S. Lin}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Graph Embedding and Extensions: A General Framework for Dimensionality Reduction}, 
year={2007}, 
volume={29}, 
number={1}, 
pages={40-51}, 
keywords={computer vision;graph theory;learning (artificial intelligence);statistical analysis;direct graph embedding;geometric property;graph extension;intraclass compactness;kernel extension;linear extension;manifold learning;marginal Fisher analysis;penalty graph;scale normalization;statistical property;subspace learning;supervised dimensionality reduction;tensor extension;Algorithm design and analysis;Face recognition;Geometry;Kernel;Laplace equations;Linear discriminant analysis;Principal component analysis;Statistics;Tensile stress;Vectors;Dimensionality reduction;graph embedding framework.;manifold learning;subspace learning;Algorithms;Artificial Intelligence;Biometry;Discriminant Analysis;Face;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity}, 
doi={10.1109/TPAMI.2007.250598}, 
ISSN={0162-8828}, 
month={Jan},}

@InProceedings{10.1007/978-3-662-44848-9_34,
author="Gulcehre, Caglar
and Cho, Kyunghyun
and Pascanu, Razvan
and Bengio, Yoshua",
editor="Calders, Toon
and Esposito, Floriana
and Hullermeier, Eyke
and Meo, Rosa",
title="Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="530--546",
abstract="In this paper we propose and investigate a novel nonlinear unit, called Lp unit, for deep neural networks. The proposed Lp unit receives signals from several projections of a subset of units in the layer below and computes a normalized L                                  p                 norm. We notice two interesting interpretations of the L                                  p                 unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the L                                  p                 unit is, to a certain degree, similar to the recently proposed maxout unit [13] which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the L                                  p                 unit is more efficient at representing complex, nonlinear separating boundaries. Each L                                  p                 unit defines a superelliptic boundary, with its exact shape defined by the order p. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few L                                  p                 units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed L                                  p                 units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the L                                  p                 units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed Lp unit on the recently proposed deep recurrent neural networks (RNN).",
isbn="978-3-662-44848-9"
}